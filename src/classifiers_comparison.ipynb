{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import facenet\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle, pdb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Input, Reshape, Dense, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "data_dir = \"../datasets/lfw/lfw_mtcnnpy_gt6\"\n",
    "model_dir = \"../models/20180408-102900/20180408-102900.pb\"\n",
    "batch_size = 90\n",
    "image_size = 160\n",
    "\n",
    "def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class, nrof_test_images_per_class):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for cls in dataset:\n",
    "        paths = cls.image_paths\n",
    "        # Remove classes with less than min_nrof_images_per_class\n",
    "        if len(paths)>=min_nrof_images_per_class:\n",
    "            np.random.shuffle(paths)\n",
    "            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n",
    "            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:nrof_train_images_per_class+nrof_test_images_per_class]))\n",
    "    return train_set, test_set\n",
    "\n",
    "def get_embeddings(dataset):\n",
    "    paths, labels = facenet.get_image_paths_and_labels(dataset)\n",
    "    print('Number of classes: %d' % len(dataset))\n",
    "    print('Number of images: %d' % len(paths))\n",
    "\n",
    "    # Load the model\n",
    "    print('Loading feature extraction model')\n",
    "    facenet.load_model(model_dir)\n",
    "\n",
    "    # Get input and output tensors\n",
    "    images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "    phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "    embedding_size = embeddings.get_shape()[1]\n",
    "\n",
    "    # Run forward pass to calculate embeddings\n",
    "    print('Calculating features for images')\n",
    "    nrof_images = len(paths)\n",
    "    nrof_batches_per_epoch = int(math.ceil(1.0*nrof_images / batch_size))\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "\n",
    "    for i in range(nrof_batches_per_epoch):\n",
    "        start_index = i*batch_size\n",
    "        end_index = min((i+1)*batch_size, nrof_images)\n",
    "        paths_batch = paths[start_index:end_index]\n",
    "        images = facenet.load_data(paths_batch, False, False, image_size)\n",
    "        feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "        emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    return emb_array, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 217\n",
      "Number of images: 1519\n",
      "Loading feature extraction model\n",
      "Model filename: ../models/20180408-102900/20180408-102900.pb\n",
      "Calculating features for images\n",
      "Number of classes: 217\n",
      "Number of images: 217\n",
      "Loading feature extraction model\n",
      "Model filename: ../models/20180408-102900/20180408-102900.pb\n",
      "Calculating features for images\n"
     ]
    }
   ],
   "source": [
    "## Calculate face embeddings from dataset\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        dataset = facenet.get_dataset(data_dir)\n",
    "        train_set, test_set = split_dataset(dataset, 8, 7, 1)\n",
    "\n",
    "        train_embs, train_labels = get_embeddings(train_set)\n",
    "        test_embs, test_labels = get_embeddings(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of classes: 901\n",
      "Nb of train faces per class: 2.0\n",
      "Nb of test faces per class: 1.0\n"
     ]
    }
   ],
   "source": [
    "# get a specific number of training images per person\n",
    "def split_train_dataset(nb_images_per_person):\n",
    "    i, train_embs, train_labels = 0, [], []\n",
    "    for l in range(len(orig_train_labels)):\n",
    "        if (l+1 < len(orig_train_labels) and orig_train_labels[l+1] != orig_train_labels[l]) or l+1 == len(orig_train_labels):\n",
    "            train_embs.extend(orig_train_embs[i:i+nb_images_per_person])\n",
    "            train_labels.extend(orig_train_labels[i:i+nb_images_per_person])\n",
    "            i = l+1\n",
    "    return np.array(train_embs), train_labels\n",
    "\n",
    "# read train/test embs/labels from pickle file\n",
    "with open(\"lfw_embs/2_per_face.pkl\", 'rb') as handle:\n",
    "    orig_train_embs, orig_train_labels, test_embs, test_labels, label_embs = pickle.load(handle)\n",
    "\n",
    "assert len(set(orig_train_labels)) == len(set(test_labels))\n",
    "\n",
    "train_embs, train_labels = split_train_dataset(nb_images_per_person=2)\n",
    "\n",
    "print(\"Nb of classes:\", len(set(test_labels)))\n",
    "print(\"Nb of train faces per class:\", len(train_labels)/len(set(test_labels)))\n",
    "print(\"Nb of test faces per class:\", len(test_labels) / len(set(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9994450610432852\n",
      "Test accuracy: 0.9400665926748057\n"
     ]
    }
   ],
   "source": [
    "get_euclidean_acc(train_embs, train_labels, test_embs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get accuracy of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=1000, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='linear', probability=True)#, gamma=0.001)\n",
    "model = MLPClassifier(hidden_layer_sizes=(1000))\n",
    "model.fit(train_embs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9994450610432852\n",
      "Test accuracy: 0.904550499445061\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_proba(train_embs)\n",
    "print(\"Train accuracy:\", model.score(train_embs, train_labels))\n",
    "\n",
    "predictions = model.predict_proba(test_embs)\n",
    "print(\"Test accuracy:\", np.mean(np.equal(np.argmax(predictions, axis=1), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create a list of class names\n",
    "# class_names = [ cls.name.replace('_', ' ') for cls in dataset]\n",
    "# # Saving classifier model\n",
    "# with open(\"lwf.pkl\", 'wb') as outfile:\n",
    "#     pickle.dump((model, class_names), outfile)\n",
    "# print('Saved classifier model to file \"%s\"' % 'lwf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get accuracy of the average euclidean distance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_embs = []\n",
    "\n",
    "def get_euclidean_acc(train_embs, train_labels, test_embs, test_labels):\n",
    "    i, label_embs = 0, []\n",
    "    while i < len(train_labels):\n",
    "        label_emb = np.copy(train_embs[i])\n",
    "        init_pos = i\n",
    "        while i+1 < len(train_labels) and train_labels[i] == train_labels[i+1]:\n",
    "            label_emb += train_embs[i+1]\n",
    "            i+=1\n",
    "        label_emb /= (i - init_pos + 1)\n",
    "        # normalize the vector\n",
    "        label_embs.append(label_emb / np.linalg.norm(label_emb))\n",
    "        i+=1\n",
    "    label_embs = np.array(label_embs)\n",
    "\n",
    "    for embs, labels, name in zip([train_embs, test_embs], [train_labels, test_labels], ['Train', 'Test']):\n",
    "        predictions = np.argmax(embs.dot(label_embs.T), axis=1)\n",
    "        accuracy = np.mean(np.equal(predictions, labels))\n",
    "        print(name, \"accuracy:\", str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 29,  85,  98, 105, 113, 118, 126, 129, 134, 135, 143, 152, 160,\n",
       "        163, 168, 175, 194, 215, 222, 226, 237, 238, 249, 252, 277, 284,\n",
       "        297, 327, 342, 343, 369, 381, 394, 409, 437, 453, 458, 463, 472,\n",
       "        483, 485, 492, 503, 506, 527, 553, 559, 563, 599, 607, 608, 613,\n",
       "        634, 673, 708, 748, 764, 765, 777, 791, 795, 797, 804, 807, 836,\n",
       "        838, 849, 860, 899]),)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((test_embs.dot(label_embs.T).argmax(axis=1) == test_labels) == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_embs = np.loadtxt('lfw_embs/2_per_face_train.out')\n",
    "# test_embs = np.loadtxt('lfw_embs/2_per_face_test.out')\n",
    "# label_embs = np.loadtxt('lfw_embs/2_per_face_train_avg.out')\n",
    "# train_embs = scaler.fit_transform(train_embs)\n",
    "# test_embs = scaler.fit_transform(test_embs)\n",
    "# label_embs = scaler.fit_transform(label_embs)\n",
    "\n",
    "X_train, y_train = train_embs, to_categorical(train_labels)\n",
    "X_test, y_test = test_embs, to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(512,))\n",
    "x = Dense(1000, activation='relu')(inputs)\n",
    "x = Dropout(0.9)(x)\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dropout(0.9)(x)\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dropout(0.9)(x)\n",
    "x = Dense(1000, activation='relu')(inputs)\n",
    "predictions = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1802/1802 [==============================] - 1s 388us/step - loss: 7.5124e-07 - acc: 1.0000\n",
      "Epoch 2/15\n",
      "1802/1802 [==============================] - 1s 336us/step - loss: 7.3566e-07 - acc: 1.0000\n",
      "Epoch 3/15\n",
      "1802/1802 [==============================] - 1s 336us/step - loss: 7.2511e-07 - acc: 1.0000\n",
      "Epoch 4/15\n",
      "1802/1802 [==============================] - 1s 338us/step - loss: 7.0785e-07 - acc: 1.0000\n",
      "Epoch 5/15\n",
      "1802/1802 [==============================] - 1s 329us/step - loss: 6.9667e-07 - acc: 1.0000\n",
      "Epoch 6/15\n",
      "1802/1802 [==============================] - 1s 333us/step - loss: 6.8850e-07 - acc: 1.0000\n",
      "Epoch 7/15\n",
      "1802/1802 [==============================] - 1s 340us/step - loss: 6.7656e-07 - acc: 1.0000\n",
      "Epoch 8/15\n",
      "1802/1802 [==============================] - 1s 335us/step - loss: 6.5946e-07 - acc: 1.0000\n",
      "Epoch 9/15\n",
      "1802/1802 [==============================] - 1s 337us/step - loss: 6.5013e-07 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "1802/1802 [==============================] - 1s 338us/step - loss: 6.3633e-07 - acc: 1.0000\n",
      "Epoch 11/15\n",
      "1802/1802 [==============================] - 1s 377us/step - loss: 6.2314e-07 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "1802/1802 [==============================] - 1s 367us/step - loss: 6.1440e-07 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "1802/1802 [==============================] - 1s 452us/step - loss: 6.0485e-07 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "1802/1802 [==============================] - 1s 443us/step - loss: 5.9472e-07 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "1802/1802 [==============================] - 1s 382us/step - loss: 5.8599e-07 - acc: 1.0000\n",
      "901/901 [==============================] - 0s 154us/step\n",
      "Test score: 0.3119645871544519 \n",
      "Test accuracy: 0.9211986682126577\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=15, batch_size=64)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test)#,batch_size=batch_size)\n",
    "print('Test score:', score, '\\nTest accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327 µs ± 54.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model.predict(X_train[1].reshape(1,-1)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classifier model to file \"<_io.BufferedWriter name='lfw_embs/7_per_face.pkl'>\"\n"
     ]
    }
   ],
   "source": [
    "# # write pickle file\n",
    "# with open(\"lfw_embs/7_per_face.pkl\", 'wb') as outfile:\n",
    "#     pickle.dump((train_embs, train_labels, test_embs, test_labels, label_embs), outfile)\n",
    "# print('Saved classifier model to file \"%s\"' % outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
